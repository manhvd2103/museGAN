{"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"gpuClass":"standard","language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/salu133445/ismir2019tutorial/blob/main/musegan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"markdown","source":"# Generating Music with GANs","metadata":{"id":"qgXVFl5SmZvC"}},{"cell_type":"markdown","source":"## Prerequisites","metadata":{"id":"mArAbLs2mBhd"}},{"cell_type":"markdown","source":"### Install dependencies","metadata":{"id":"aeGaPSXNdqy-"}},{"cell_type":"code","source":"!pip3 install pypianoroll livelossplot","metadata":{"id":"PQz39McPVHUa","vscode":{"languageId":"python"},"execution":{"iopub.status.busy":"2023-03-29T01:43:38.291986Z","iopub.execute_input":"2023-03-29T01:43:38.292308Z","iopub.status.idle":"2023-03-29T01:43:55.336629Z","shell.execute_reply.started":"2023-03-29T01:43:38.292266Z","shell.execute_reply":"2023-03-29T01:43:55.335407Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pypianoroll\n  Downloading pypianoroll-1.0.4-py3-none-any.whl (26 kB)\nCollecting livelossplot\n  Downloading livelossplot-0.5.5-py3-none-any.whl (22 kB)\nCollecting pretty-midi>=0.2.8\n  Downloading pretty_midi-0.2.10.tar.gz (5.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: scipy>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from pypianoroll) (1.7.3)\nRequirement already satisfied: matplotlib>=1.5 in /opt/conda/lib/python3.7/site-packages (from pypianoroll) (3.5.3)\nRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from pypianoroll) (1.21.6)\nRequirement already satisfied: ipython==7.* in /opt/conda/lib/python3.7/site-packages (from livelossplot) (7.34.0)\nRequirement already satisfied: bokeh in /opt/conda/lib/python3.7/site-packages (from livelossplot) (2.4.3)\nRequirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython==7.*->livelossplot) (5.8.1)\nRequirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython==7.*->livelossplot) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython==7.*->livelossplot) (3.0.36)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from ipython==7.*->livelossplot) (4.8.0)\nRequirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.7/site-packages (from ipython==7.*->livelossplot) (0.1.6)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.7/site-packages (from ipython==7.*->livelossplot) (0.18.2)\nRequirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython==7.*->livelossplot) (2.14.0)\nRequirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython==7.*->livelossplot) (0.2.0)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython==7.*->livelossplot) (5.1.1)\nRequirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from ipython==7.*->livelossplot) (59.8.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=1.5->pypianoroll) (23.0)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=1.5->pypianoroll) (9.4.0)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=1.5->pypianoroll) (2.8.2)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=1.5->pypianoroll) (0.11.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=1.5->pypianoroll) (1.4.4)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=1.5->pypianoroll) (3.0.9)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=1.5->pypianoroll) (4.38.0)\nCollecting mido>=1.1.16\n  Downloading mido-1.2.10-py2.py3-none-any.whl (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from pretty-midi>=0.2.8->pypianoroll) (1.16.0)\nRequirement already satisfied: tornado>=5.1 in /opt/conda/lib/python3.7/site-packages (from bokeh->livelossplot) (6.1)\nRequirement already satisfied: Jinja2>=2.9 in /opt/conda/lib/python3.7/site-packages (from bokeh->livelossplot) (3.1.2)\nRequirement already satisfied: typing-extensions>=3.10.0 in /opt/conda/lib/python3.7/site-packages (from bokeh->livelossplot) (4.4.0)\nRequirement already satisfied: PyYAML>=3.10 in /opt/conda/lib/python3.7/site-packages (from bokeh->livelossplot) (6.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.16->ipython==7.*->livelossplot) (0.8.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from Jinja2>=2.9->bokeh->livelossplot) (2.1.1)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->ipython==7.*->livelossplot) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython==7.*->livelossplot) (0.2.6)\nBuilding wheels for collected packages: pretty-midi\n  Building wheel for pretty-midi (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pretty-midi: filename=pretty_midi-0.2.10-py3-none-any.whl size=5592305 sha256=9ae93ef1b01db916194cdbbd34eed8eae1561ff17e23733ea306ba4fcbf127b0\n  Stored in directory: /root/.cache/pip/wheels/ea/8a/5a/615ed5b0cab54051df02fbaa4ad5334f91e3156b8c8753f5fc\nSuccessfully built pretty-midi\nInstalling collected packages: mido, pretty-midi, pypianoroll, livelossplot\nSuccessfully installed livelossplot-0.5.5 mido-1.2.10 pretty-midi-0.2.10 pypianoroll-1.0.4\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"### Import libraries","metadata":{"id":"Bn-74oKmdyF7"}},{"cell_type":"code","source":"from IPython.display import clear_output\nfrom ipywidgets import interact, IntSlider\n\nimport os\nimport os.path\nimport random\nfrom pathlib import Path\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport pypianoroll\nfrom pypianoroll import Multitrack, Track, BinaryTrack\nfrom tqdm import tqdm\nfrom livelossplot import PlotLosses\nfrom livelossplot.outputs import MatplotlibPlot","metadata":{"id":"zOkT9h38krfZ","vscode":{"languageId":"python"},"execution":{"iopub.status.busy":"2023-03-29T01:43:55.340072Z","iopub.execute_input":"2023-03-29T01:43:55.340735Z","iopub.status.idle":"2023-03-29T01:43:59.857516Z","shell.execute_reply.started":"2023-03-29T01:43:55.340698Z","shell.execute_reply":"2023-03-29T01:43:59.856501Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Settings","metadata":{"id":"6VyXuXFtoLxL"}},{"cell_type":"code","source":"# Data\nn_tracks = 4  # number of tracks\nn_pitches = 72  # number of pitches\nlowest_pitch = 24  # MIDI note number of the lowest pitch\nn_samples_per_song = 4  # number of samples to extract from each song in the datset\nn_measures = 4  # number of measures per sample\nbeat_resolution = 4  # temporal resolution of a beat (in timestep)\nprograms = [0, 0, 25, 33]  # program number for each track\nis_drums = [True, False, False, False]  # drum indicator for each track\ntrack_names = ['Drums', 'Piano', 'Guitar', 'Bass']  # name of each track\ntempo = 70\n\n# Training\nbatch_size = 16\nlatent_dim = 128\nn_steps = 50000\n\n# Sampling\nsample_interval = 500  # interval to run the sampler (in step)\nn_samples = 4","metadata":{"id":"PA14sQ-YoTvW","vscode":{"languageId":"python"},"execution":{"iopub.status.busy":"2023-03-29T01:43:59.859207Z","iopub.execute_input":"2023-03-29T01:43:59.859954Z","iopub.status.idle":"2023-03-29T01:43:59.868039Z","shell.execute_reply.started":"2023-03-29T01:43:59.859913Z","shell.execute_reply":"2023-03-29T01:43:59.866759Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"measure_resolution = 4 * beat_resolution\ntempo_array = np.full((4 * 4 * measure_resolution, 1), tempo)\nassert 24 % beat_resolution == 0, (\n    \"beat_resolution must be a factor of 24 (the beat resolution used in \"\n    \"the source dataset).\"\n)\nassert len(programs) == len(is_drums) and len(programs) == len(track_names), (\n    \"Lengths of programs, is_drums and track_names must be the same.\"\n)    ","metadata":{"id":"F91IBYd8mr9f","vscode":{"languageId":"python"},"execution":{"iopub.status.busy":"2023-03-29T01:43:59.871573Z","iopub.execute_input":"2023-03-29T01:43:59.871996Z","iopub.status.idle":"2023-03-29T01:43:59.882480Z","shell.execute_reply.started":"2023-03-29T01:43:59.871929Z","shell.execute_reply":"2023-03-29T01:43:59.881379Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Data Prepration","metadata":{"id":"-JyxQDtXfhbp"}},{"cell_type":"code","source":"data = []\nroot_dir = '/kaggle/input/cleaned-musegan-data/cleaned_data'\nsong_list = os.listdir(root_dir)\nprint(len(song_list))\nfor idx in tqdm(range(len(song_list))):\n    multitrack = pypianoroll.load(os.path.join(root_dir,song_list[idx]))\n    for track in multitrack.tracks:\n        track_name = track.name.split(' ')\n        track_name = [name.lower() for name in track_name]\n        if 'drums' in track_name or 'bass' in track_name or 'guitar' in track_name or 'piano' in track_name:\n            continue\n        multitrack.tracks.remove(track)\n    if len(multitrack.tracks) < 4:\n        continue\n    multitrack.tracks = multitrack.tracks[0:4]\n    multitrack.binarize()\n    multitrack.set_resolution(beat_resolution)\n    pianoroll= (multitrack.stack() > 0)\n    pianoroll = pianoroll[:, :, lowest_pitch: lowest_pitch + n_pitches]\n    n_total_measures = multitrack.get_max_length() // (4 * n_measures)\n    candidate = n_total_measures - n_measures\n    target_n_samples = min(n_total_measures // n_measures, n_samples_per_song)\n    if target_n_samples > 0:\n        for i in np.random.choice(candidate, target_n_samples, False):\n            start = i * 4 * beat_resolution\n            end = (i + n_measures) * 4 * beat_resolution\n            if (pianoroll.sum(axis=(1, 2)) < 10).any():\n                continue\n    else:\n        continue\n    data.append(torch.as_tensor(pianoroll[:, start:end], dtype=torch.float32))","metadata":{"execution":{"iopub.status.busy":"2023-03-29T01:43:59.884186Z","iopub.execute_input":"2023-03-29T01:43:59.884570Z","iopub.status.idle":"2023-03-29T01:53:49.828609Z","shell.execute_reply.started":"2023-03-29T01:43:59.884535Z","shell.execute_reply":"2023-03-29T01:53:49.827521Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"14410\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 14410/14410 [09:48<00:00, 24.48it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"class MuseGANDataset(Dataset):\n\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return data[idx]\n\n\ndataset = MuseGANDataset(data)\ndata_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)","metadata":{"id":"wwg8m1NQziKo","execution":{"iopub.status.busy":"2023-03-29T01:53:49.830395Z","iopub.execute_input":"2023-03-29T01:53:49.830766Z","iopub.status.idle":"2023-03-29T01:53:49.838814Z","shell.execute_reply.started":"2023-03-29T01:53:49.830727Z","shell.execute_reply":"2023-03-29T01:53:49.837851Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{"id":"94rrn1nmIQlG"}},{"cell_type":"markdown","source":" ### Define the generator","metadata":{"id":"SyX6Duf5fkiw"}},{"cell_type":"code","source":"class GeneraterBlock(torch.nn.Module):\n    def __init__(self, in_dim, out_dim, kernel, stride):\n        super().__init__()\n        self.transconv = torch.nn.ConvTranspose3d(in_dim, out_dim, kernel, stride)\n        self.batchnorm = torch.nn.BatchNorm3d(out_dim)\n    \n    def forward(self, x):\n        x = self.transconv(x)\n        x = self.batchnorm(x)\n        return torch.nn.functional.relu(x)","metadata":{"id":"Fj9sCKbSKcse","vscode":{"languageId":"python"},"execution":{"iopub.status.busy":"2023-03-29T01:53:49.840597Z","iopub.execute_input":"2023-03-29T01:53:49.841060Z","iopub.status.idle":"2023-03-29T01:53:49.863892Z","shell.execute_reply.started":"2023-03-29T01:53:49.841019Z","shell.execute_reply":"2023-03-29T01:53:49.862801Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\nclass Generator(torch.nn.Module):\n    \"\"\"A convolutional neural network (CNN) based generator. The generator takes\n    as input a latent vector and outputs a fake sample.\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.transconv0 = torch.nn.Sequential(\n            GeneraterBlock(128, 2048, (2, 1, 1), (2, 1, 1)),\n            GeneraterBlock(2048, 1024, (2, 1, 1), (2, 1, 1)),\n            GeneraterBlock(1024, 512, (1, 2, 1), (1, 2, 1)),\n            GeneraterBlock(512, 256, (1, 2, 1), (1, 2, 1)),\n            GeneraterBlock(256, 128, (1, 1, 2), (1, 1, 2)),\n            GeneraterBlock(128, 64, (1, 1, 2), (1, 1, 2)),\n            GeneraterBlock(64, 32, (1, 1, 3), (1, 1, 1))\n        )\n        self.transconv1 = torch.nn.ModuleList([\n            GeneraterBlock(32, 16, (1, 4, 1), (1, 4, 1))\n            for _ in range(4)\n        ])\n        self.transconv2 = torch.nn.ModuleList([\n            torch.nn.Sequential(\n                torch.nn.ConvTranspose3d(16, 1, (1, 1, 12), (1, 1, 12)),\n                torch.nn.Tanh()\n            )\n            for _ in range(4)\n        ])\n\n    def forward(self, x):\n        x = x.view(-1, 128, 1, 1, 1)\n        x = self.transconv0(x)\n        x = [transconv(x) for transconv in self.transconv1]\n        x = torch.cat([transconv(x_) for x_, transconv in zip(x, self.transconv2)], 1)\n        x = x.view(-1, 4, 4 * 16, 72)\n        return x","metadata":{"id":"ZWPAxfkmsIWn","vscode":{"languageId":"python"},"execution":{"iopub.status.busy":"2023-03-29T01:53:49.865187Z","iopub.execute_input":"2023-03-29T01:53:49.865988Z","iopub.status.idle":"2023-03-29T01:53:49.887709Z","shell.execute_reply.started":"2023-03-29T01:53:49.865951Z","shell.execute_reply":"2023-03-29T01:53:49.886691Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":" ### Define the discriminator","metadata":{"id":"cJPGeYNbfvw6"}},{"cell_type":"code","source":"class LayerNorm(torch.nn.Module):\n    \"\"\"An implementation of Layer normalization that does not require size\n    information. Copied from https://github.com/pytorch/pytorch/issues/1959.\"\"\"\n    def __init__(self, n_features, eps=1e-5, affine=True):\n        super().__init__()\n        self.n_features = n_features\n        self.affine = affine\n        self.eps = eps\n        if self.affine:\n            self.gamma = torch.nn.Parameter(torch.Tensor(n_features).uniform_())\n            self.beta = torch.nn.Parameter(torch.zeros(n_features))\n\n    def forward(self, x):\n        shape = [-1] + [1] * (x.dim() - 1)\n        mean = x.view(x.size(0), -1).mean(1).view(*shape)\n        std = x.view(x.size(0), -1).std(1).view(*shape)\n        y = (x - mean) / (std + self.eps)\n        if self.affine:\n            shape = [1, -1] + [1] * (x.dim() - 2)\n            y = self.gamma.view(*shape) * y + self.beta.view(*shape)\n        return y","metadata":{"id":"Knw5u6Px2c8j","vscode":{"languageId":"python"},"execution":{"iopub.status.busy":"2023-03-29T01:53:49.889039Z","iopub.execute_input":"2023-03-29T01:53:49.889500Z","iopub.status.idle":"2023-03-29T01:53:49.900538Z","shell.execute_reply.started":"2023-03-29T01:53:49.889460Z","shell.execute_reply":"2023-03-29T01:53:49.899526Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class DiscriminatorBlock(torch.nn.Module):\n    def __init__(self, in_dim, out_dim, kernel, stride):\n        super().__init__()\n        self.transconv = torch.nn.Conv3d(in_dim, out_dim, kernel, stride)\n        self.layernorm = LayerNorm(out_dim)\n        self.dropout = torch.nn.Dropout(p=0.1)\n    def forward(self, x):\n        x = self.transconv(x)\n        x = self.layernorm(x)\n        x = self.dropout(x)\n        return torch.nn.functional.leaky_relu(x)","metadata":{"id":"gZhbO2jiFLG5","vscode":{"languageId":"python"},"execution":{"iopub.status.busy":"2023-03-29T01:53:49.905128Z","iopub.execute_input":"2023-03-29T01:53:49.905385Z","iopub.status.idle":"2023-03-29T01:53:49.916372Z","shell.execute_reply.started":"2023-03-29T01:53:49.905360Z","shell.execute_reply":"2023-03-29T01:53:49.915383Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class Discriminator(torch.nn.Module):\n    \"\"\"A convolutional neural network (CNN) based discriminator. The\n    discriminator takes as input either a real sample (in the training data) or\n    a fake sample (generated by the generator) and outputs a scalar indicating\n    its authentity.\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.conv0 = torch.nn.ModuleList([\n            DiscriminatorBlock(1, 16, (1, 1, 12), (1, 1, 12)) for _ in range(4)\n        ])\n        self.conv1 = torch.nn.ModuleList([\n            DiscriminatorBlock(16, 16, (1, 4, 1), (1, 4, 1)) for _ in range(4)\n        ])\n        self.conv2 = torch.nn.Sequential(\n            DiscriminatorBlock(16 * 4, 64, (1, 1, 3), (1, 1, 1)),\n            DiscriminatorBlock(64, 128, (1, 1, 2), (1, 1, 2)),\n            DiscriminatorBlock(128, 128, (1, 1, 2), (1, 1, 2)),\n            DiscriminatorBlock(128, 256, (1, 2, 1), (1, 2, 1)),\n            DiscriminatorBlock(256, 256, (1, 2, 1), (1, 2, 1)),\n            DiscriminatorBlock(256, 512, (2, 1, 1), (1, 1, 1)),\n            DiscriminatorBlock(512, 512, (3, 1, 1), (3, 1, 1))\n        )\n        self.dense = torch.nn.Linear(512, 1)\n\n    def forward(self, x):\n        #print('d_input', x.shape)\n        x = x.view(-1, 4, 4, 16, 72)\n        x = [conv(x[:, [i]]) for i, conv in enumerate(self.conv0)]\n        x = torch.cat([conv(x_) for x_, conv in zip(x, self.conv1)], 1)\n        x = self.conv2(x)\n        x = x.view(-1, 512)\n        x = self.dense(x)\n        return x\n     \n","metadata":{"id":"kczm8A8Nl78i","vscode":{"languageId":"python"},"execution":{"iopub.status.busy":"2023-03-29T01:53:49.918584Z","iopub.execute_input":"2023-03-29T01:53:49.918889Z","iopub.status.idle":"2023-03-29T01:53:49.932140Z","shell.execute_reply.started":"2023-03-29T01:53:49.918850Z","shell.execute_reply":"2023-03-29T01:53:49.931271Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Training functions","metadata":{"id":"IiPl8DYCI7pC"}},{"cell_type":"code","source":"def compute_gradient_penalty(discriminator, real_samples, fake_samples):\n    \"\"\"Compute the gradient penalty for regularization. Intuitively, the\n    gradient penalty help stablize the magnitude of the gradients that the\n    discriminator provides to the generator, and thus help stablize the training\n    of the generator.\"\"\"\n    # Get random interpolations between real and fake samples\n    alpha = torch.rand(real_samples.size(0), 1, 1, 1).cuda()\n    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples))\n    interpolates = interpolates.requires_grad_(True)\n    # Get the discriminator output for the interpolations\n    d_interpolates = discriminator(interpolates)\n    # Get gradients w.r.t. the interpolations\n    fake = torch.ones(real_samples.size(0), 1).cuda()\n    gradients = torch.autograd.grad(\n        outputs=d_interpolates,\n        inputs=interpolates,\n        grad_outputs=fake,  \n        create_graph=True,\n        retain_graph=True,\n        only_inputs=True\n    )[0]\n    # Compute gradient penalty\n    gradients = gradients.view(gradients.size(0), -1)\n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n    return gradient_penalty","metadata":{"id":"5wngyfaaObas","vscode":{"languageId":"python"},"execution":{"iopub.status.busy":"2023-03-29T01:53:49.933663Z","iopub.execute_input":"2023-03-29T01:53:49.934332Z","iopub.status.idle":"2023-03-29T01:53:49.950531Z","shell.execute_reply.started":"2023-03-29T01:53:49.934296Z","shell.execute_reply":"2023-03-29T01:53:49.949613Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def train_one_step(d_optimizer, g_optimizer, real_samples):\n    \"\"\"Train the networks for one step.\"\"\"\n    # Sample from the lantent distribution\n    latent = torch.rand(batch_size, latent_dim)\n\n    # Transfer data to GPU\n    if torch.cuda.is_available():\n        real_samples = real_samples .cuda()\n        latent = latent.cuda()\n    \n    # === Train the discriminator ===\n    for _ in range(5):\n        # Reset cached gradients to zero\n        d_optimizer.zero_grad()\n        # Get discriminator outputs for the real samples\n        prediction_real = discriminator(real_samples)\n        # Compute the loss function\n        # d_loss_real = torch.mean(torch.nn.functional.relu(1. - prediction_real))\n        d_loss_real = -torch.mean(prediction_real)\n        # Backpropagate the gradients\n        #d_loss_real.backward()\n        #print('d_loss_real', d_loss_real.shape)\n        \n        # Generate fake samples with the generator\n        fake_samples = generator(latent)\n        # Get discriminator outputs for the fake samples\n        prediction_fake_d = discriminator(fake_samples.detach())\n        # Compute the loss function\n        # d_loss_fake = torch.mean(torch.nn.functional.relu(1. + prediction_fake_d))\n        d_loss_fake = torch.mean(prediction_fake_d)\n        # Backpropagate the gradients\n        #d_loss_fake.backward()\n        #print('d_loss_fake', d_loss_fake.shape)\n\n        # Compute gradient penalty\n        gradient_penalty = 10.0 * compute_gradient_penalty(\n            discriminator, real_samples.data, fake_samples.data\n        )\n        # Backpropagate the gradients\n        #gradient_penalty.backward()\n        #print('gradient_penalty', gradient_penalty.shape)\n        d_loss = d_loss_real + d_loss_fake + gradient_penalty\n        d_loss.backward()\n\n        # Update the weights\n        d_optimizer.step()\n    \n    # === Train the generator ===\n    # Reset cached gradients to zero\n    g_optimizer.zero_grad()\n    # Get discriminator outputs for the fake samples\n    prediction_fake_g = discriminator(fake_samples)\n    # Compute the loss function\n    g_loss = -torch.mean(prediction_fake_g)\n    # Backpropagate the gradients\n    g_loss.backward()\n    # Update the weights\n    g_optimizer.step()\n\n    return d_loss, g_loss","metadata":{"id":"x3mgXtVN8ldM","vscode":{"languageId":"python"},"execution":{"iopub.status.busy":"2023-03-29T01:53:49.952107Z","iopub.execute_input":"2023-03-29T01:53:49.952491Z","iopub.status.idle":"2023-03-29T01:53:49.968126Z","shell.execute_reply.started":"2023-03-29T01:53:49.952434Z","shell.execute_reply":"2023-03-29T01:53:49.967181Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Training Preparation","metadata":{"id":"4ukfh1dxIsDw"}},{"cell_type":"code","source":"# Create data loader\n# data_loader = get_data_loader()\n\n# Create neural networks\ndiscriminator = Discriminator()\ngenerator = Generator()\nprint(\"Number of parameters in G: {}\".format(\n    sum(p.numel() for p in generator.parameters() if p.requires_grad)))\nprint(\"Number of parameters in D: {}\".format(\n    sum(p.numel() for p in discriminator.parameters() if p.requires_grad)))\n\n# Create optimizers\nd_optimizer = torch.optim.Adam(\n    discriminator.parameters(), lr=0.001,  betas=(0.5, 0.9))\ng_optimizer = torch.optim.Adam(\n    generator.parameters(), lr=0.001, betas=(0.5, 0.9))\n\n# Prepare the inputs for the sampler, which wil run during the training\nsample_latent = torch.rand(n_samples, latent_dim)\n\n# Transfer the neural nets and samples to GPU\nif torch.cuda.is_available():\n    discriminator = discriminator.cuda()\n    generator = generator.cuda()\n    sample_latent = sample_latent.cuda()\n\n# Create an empty dictionary to sotre history samples\nhistory_samples = {}\n\n# Create a LiveLoss logger instance for monitoring\nliveloss = PlotLosses(outputs=[MatplotlibPlot(cell_size=(6,2))])\n\n# Initialize step\nstep = 0","metadata":{"id":"uCqTBe3p09xY","vscode":{"languageId":"python"},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1db1b2d2-04ef-447e-aa9f-c3ac7fa2ee37","execution":{"iopub.status.busy":"2023-03-29T01:53:49.969436Z","iopub.execute_input":"2023-03-29T01:53:49.970051Z","iopub.status.idle":"2023-03-29T01:53:54.283690Z","shell.execute_reply.started":"2023-03-29T01:53:49.970006Z","shell.execute_reply":"2023-03-29T01:53:54.282540Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Number of parameters in G: 6138724\nNumber of parameters in D: 1317953\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Training","metadata":{"id":"cL499fTNJcSd"}},{"cell_type":"code","source":"print(len(dataset))","metadata":{"execution":{"iopub.status.busy":"2023-03-29T01:53:54.285424Z","iopub.execute_input":"2023-03-29T01:53:54.285874Z","iopub.status.idle":"2023-03-29T01:53:54.292633Z","shell.execute_reply.started":"2023-03-29T01:53:54.285817Z","shell.execute_reply":"2023-03-29T01:53:54.291501Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"14243\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create a progress bar instance for monitoring\nprogress_bar = tqdm(total=n_steps, initial=step, ncols=80, mininterval=1)\nbest_d_loss = 10000.0\nbest_g_loss = 10000.0\n# Start iterations\nwhile step < n_steps + 1:\n    # Iterate over the dataset\n    for real_samples in data_loader:\n        # Train the neural networks\n        generator.train()\n        d_loss, g_loss = train_one_step(d_optimizer, g_optimizer, real_samples)\n        if abs(d_loss) < best_d_loss :\n            best_d_loss = abs(d_loss)\n            generator.eval()\n            with torch.no_grad():\n                torch.save(generator, f'generator.pt') \n        if step % 500 == 0:\n            # Record smoothened loss values to LiveLoss logger\n            liveloss.update({'d_loss': d_loss, 'g_loss': g_loss})\n            \n            # Update losses to progress bar\n            progress_bar.set_description_str(\n                \"(d_loss={: 8.6f}, g_loss={: 8.6f})\".format(d_loss, g_loss)\n            )\n\n            # Get generated samples\n            generator.eval()\n            sample_latent = torch.rand(n_samples, latent_dim).cuda()\n            with torch.no_grad():\n                torch.save(generator, f'generator_{step}.pt')\n                samples = generator(sample_latent).cpu().detach().numpy()\n            history_samples[step] = samples\n\n            # Display loss curves\n            clear_output(True)\n            \n            # Display generated samples\n            samples = samples.transpose(1, 0, 2, 3).reshape(n_tracks, -1, n_pitches)\n            tracks = []\n            for idx, (program, is_drum, track_name) in enumerate(\n                zip(programs, is_drums, track_names)\n            ):\n                pianoroll = np.pad(\n                    samples[idx] > 0.5,\n                    ((0, 0), (lowest_pitch, 128 - lowest_pitch - n_pitches))\n                )\n                tracks.append(\n                    BinaryTrack(\n                        name=track_name,\n                        program=program,\n                        is_drum=is_drum,\n                        pianoroll=pianoroll\n                    )\n                )\n            m = Multitrack(\n                tracks=tracks,\n                tempo=tempo_array,\n                resolution=beat_resolution\n            )\n            m.write(f'./train_v3_{step}.mid') # download generated melody\n            plt.show()\n            \n        step += 1\n        progress_bar.update(1)\n        if step >= n_steps:\n            break","metadata":{"id":"JsCO34_A3N2U","vscode":{"languageId":"python"},"colab":{"base_uri":"https://localhost:8080/","height":415},"outputId":"c3196ec0-eb95-46c7-c40c-bb358038a4d0","execution":{"iopub.status.busy":"2023-03-29T01:53:54.294477Z","iopub.execute_input":"2023-03-29T01:53:54.294965Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"(d_loss=-6.820445, g_loss=-1.531428):  13%|▏| 6417/50000 [45:38<5:02:30,  2.40it","output_type":"stream"}]},{"cell_type":"markdown","source":"## Training History","metadata":{"id":"UjX855uYf6sC"}},{"cell_type":"code","source":"# Show history\nsteps = [5000, 10000, 15000, 20000]\nfor step in steps:\n    print(f\"Step={step}\")\n    samples = history_samples[step].transpose(1, 0, 2, 3).reshape(n_tracks, -1, n_pitches)\n    tracks = []\n    for idx, (program, is_drum, track_name) in enumerate(zip(programs, is_drums, track_names)):\n        pianoroll = np.pad(\n            samples[idx] > 0.5,\n            ((0, 0), (lowest_pitch, 128 - lowest_pitch - n_pitches))\n        )\n        tracks.append(\n            BinaryTrack(\n                name=track_name,\n                program=program,\n                is_drum=is_drum,\n                pianoroll=pianoroll,\n            )\n        )\n    m = Multitrack(tracks=tracks, tempo=tempo_array, resolution=beat_resolution)\n\n    axs = m.plot()\n    for ax in axs:\n        for x in range(\n            measure_resolution,\n            4 * measure_resolution * n_measures,\n            measure_resolution\n        ):\n            if x % (measure_resolution * 4) == 0:\n                ax.axvline(x - 0.5, color='k')\n            else:\n                ax.axvline(x - 0.5, color='k', linestyle='-', linewidth=1)\n    plt.gcf().set_size_inches((16, 8))\n    plt.show()\nprint(best_d_loss, best_g_loss)","metadata":{"id":"XqN1sKoODaCm","vscode":{"languageId":"python"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generating a new melody","metadata":{"id":"fhj8026rzK6V"}},{"cell_type":"code","source":"with torch.no_grad():\n    generated = generator(torch.rand(batch_size, latent_dim).cuda()).cpu().detach().numpy()\n    generated = generated.transpose(1, 0, 2, 3).reshape(n_tracks, -1, n_pitches)\n\ntracks = []\nfor idx, (program, is_drum, track_name) in enumerate(zip(programs, is_drums, track_names)):\n    pianoroll = np.pad(\n        generated[idx] > 0.5,\n        ((0, 0), (lowest_pitch, 128 - lowest_pitch - n_pitches))\n    )\n    tracks.append(\n        BinaryTrack(\n            name=track_name,\n            program=program,\n            is_drum=is_drum,\n            pianoroll=pianoroll,\n        )\n    )\nm = Multitrack(tracks=tracks, tempo=tempo_array, resolution=beat_resolution)\n\nm.write(f'./train_v3_generated.mid')\n\naxs = m.plot()\nplt.gcf().set_size_inches((16, 8))\nplt.show()","metadata":{"id":"aeJpOeUXjdBu","vscode":{"languageId":"python"},"trusted":true},"execution_count":null,"outputs":[]}]}